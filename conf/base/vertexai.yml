# Configuration used to run the pipeline
project_id: gid-ml-framework
region: europe-west4
run_config:
  # Name of the image to run as the pipeline steps
  image: europe-west4-docker.pkg.dev/gid-ml-framework/gid-ml-framework-data/gid-ml-framework:latest

  # Pull policy to be used for the steps. Use Always if you push the images
  # on the same tag, or Never if you use only local images
  image_pull_policy: Always

  # Location of Vertex AI GCS root
  root: gid-ml-framework-data/temp

  # Prefix of Vertex AI pipeline run
  experiment_name: gid-ml-framework-gnns-santander

  # Name of the scheduled run, templated with the schedule parameters
  scheduled_run_name: gid-ml-framework-gnns-santander

  run_name: gid-ml-framework-gnns-santander-${run_id}

  # Optional service account to run vertex AI Pipeline with
  # service_account: iap-accessor@gid-advanced-analytics-infra.iam.gserviceaccount.com

  # Optional pipeline description
  # description: "Very Important Pipeline"

  # How long to keep underlying Argo workflow (together with pods and data
  # volume after pipeline finishes) [in seconds]. Default: 1 week
  ttl: 604800

  # Optional network configuration
  # network:

    # Name of the vpc to use for running Vertex Pipeline
    # vpc: my-vpc

    # Hosts aliases to be placed in /etc/hosts when pipeline is executed
    # host_aliases:
    #  - ip: 127.0.0.1
    #    hostnames:
    #     - me.local

  # What Kedro pipeline should be run as the last step regardless of the
  # pipeline status. Used to send notifications or raise the alerts
  # on_exit_pipeline: notify_via_slack

  # Optional section allowing adjustment of the resources, reservations and limits
  # for the nodes. When not provided they're set to 500m cpu and 1024Mi memory.
  # If you don't want to specify pipeline resources set both to None in __default__.
  resources:

    # For nodes that require more RAM you can increase the "memory"
    # data_import_step:
    #   memory: 2Gi

    # # Training nodes can utilize more than one CPU if the algoritm
    # # supports it
    santander_dgsr_kaggle_gr.train_model_node:
      nvidia.com/gpu: 1

    # Default settings for the nodes
    __default__:
      cpu: 2000m
      memory: 64Gi

  # Optional section allowing to generate config files at runtime,
  # useful e.g. when you need to obtain credentials dynamically and store them in credentials.yaml
  # but the credentials need to be refreshed per-node
  # (which in case of Vertex AI would be a separate container / machine)
  # Example:
  # dynamic_config_providers:
  #   - cls: kedro_vertexai.auth.gcp.MLFlowGoogleIAMCredentialsProvider
  #     params:
  #       client_id: 260364966542-721p48q045h4o2hmsmg5dl0gksm7udiu.apps.googleusercontent.com
  #       service_account: iap-accessor@gid-advanced-analytics-infra.iam.gserviceaccount.com
  dynamic_config_providers: []

  # MLflow IAM authorization
  mlflow:
    request_header_provider_params:
      client_id: 260364966542-721p48q045h4o2hmsmg5dl0gksm7udiu.apps.googleusercontent.com
      service_account: iap-accessor@gid-advanced-analytics-infra.iam.gserviceaccount.com

