# Graph Neural Networks for sequential data recommendations

## Overview

`recommender_gnn` is one of the use cases implemented within QuickStart ML Blueprints. The purpose of this use case is to
demonstrate the utilization of an example graph network that achieves state-of-the-art results for a recommendation task
on sequential data. It can also be used as a guide on how to train models using GPU accelerators on Google Cloud
Platform employing the [kedro-vertexai plugin](https://github.com/getindata/kedro-vertexai). This use case can be also
used as an example of Kedro project utilizing datasets from different sources. 

## Data

As this case focuses on sequential recommendation data, we can use any data of this type if it is properly preprocessed.
An example preprocessing pipeline was implemented for [OTTO â€“ Multi-Objective Recommender System](https://www.kaggle.com/competitions/otto-recommender-system)
dataset from Kaggle competition. It is also possible to utilize transaction data from [H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations) dataset, which are preprocessed as 
part of `recommender_ranking` use case. In the general case, the data we want to use as an input, should be preprocessed
in accordance to following example:

| customer_id | article_id | timestamp  |
|-------------|------------|------------|
| 1215        | shoes      | 1670405262 |
| 1220        | hat        | 1651405345 |
| 2600        | t-shirt    | 1631445222 |

which is called `transactions` table and should include data about every interaction in our recommendation dataset. Apart
from transactions these can be for example clicks generated by users or additions to website cart. Each such interaction
in this format should be described with customer (or for example session) identifier in
`customer_id` column, article identifier in `article_id` column, and time of an interaction in `timestamp` if it is an 
integer or `date` column if it is in date format. Apart from `transactions` table there should also be present
`customers` and `articles` tables with unique customers and articles identifiers respectively. In this use case such a 
format is called **ACT** (Articles, Customers, Transactions).

## Models

For now examples are implemented with use of [Dynamic Graph Neural Networks for Sequential 
Recommendation](https://github.com/ZM7/DGSR) (DGSR) (Zhang et al., [2022](https://doi.org/10.1109/TKDE.2022.3151618)). This
algorithm utilize ideas from **dynamic graph neural networks** to model the dynamic collaborative signals among all
interactions in data, which helps in exploring the interactive behavior of users and items with time and order
information. Two main parts of aforementioned network are **attention module**, which is constructed to capture the long-term
preference of users and long-term character of items, and a **recurrent neural module**, which is further utilized to learn
short-term preference and character of users and items, respectively. The above model was implemented using deep learning frameworks such as [PyTorch](https://pytorch.org/), [Deep Graph Library](https://www.dgl.ai/) and [PyTorch Lightning](https://www.pytorchlightning.ai/).

![dgsr_architecture](https://user-images.githubusercontent.com/32554595/205959876-25f6e4fa-4a32-4a79-9cff-77e8f5656077.png)


## Project Setup

### Data Setup
You can download `Otto` dataset with optimized memory footprint in parquet format from [here](https://www.kaggle.com/datasets/radek1/otto-full-optimized-memory-footprint).
Four downloaded files should be placed in `recommender_gnn/data/02_intermediate/otto` directory.

You can also use `H&M` dataset, which preprocessed samples can be found [here](https://drive.google.com/drive/folders/11FR_jIBbBkGeNvU1RvVzlcm5O3wKclpw?usp=share_link).
Three downloaded files containing data already in **ACT** format should be placed in `recommender_gnn/data/03_primary/hm` directory.

The Otto dataset is significantly larger and in order to run preprocessing on it you need a machine with at least 16 GB of RAM, this applies to both local and cloud setup. The H&M data is smaller and does not require as much RAM.

### Local and Remote Setup Using VSCode devcontainers (recommended)
This approach facilitates use of [VSCode devcontainers](https://code.visualstudio.com/docs/devcontainers/containers). It is the easiest way to set up the development environment. 

Prerequisites:
* [VSCode](https://code.visualstudio.com/) with [Remote development](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) extension
* [Docker](https://www.docker.com/) with `/workspaces` entry in `Docker Desktop > Preferences > Resources > File Sharing`

Setting up:

If you want to setup your environment in cloud first follow this [instruction](https://getindata.atlassian.net/wiki/spaces/GINT/pages/3107094529/Development+in+cloud+GCP+VertexAI+Notebooks). For the only local case you can move on right away.

1. Clone this repository.
2. Optionally, if on your machine you have access and want to use GPU acceleration uncomment NVIDIA GPU part of command in `runArgs` section in `recommender_gnn/.devcontainer/devcontainer.json` file.
3. Open the `recommender_gnn` directory [in a container](https://code.visualstudio.com/docs/devcontainers/containers#_quick-start-open-an-existing-folder-in-a-container).
4. Activate python environment for this use case:

    ```
    conda activate gnns
    ```

5. To use kedro vertexai properly you need to set up gcloud:

    ```
    gcloud auth login --update-adc
    gcloud config set project <your_gcp_project_name>
    gcloud auth configure-docker europe-west4-docker.pkg.dev
    ```

6. You're good to go!

## Kedro Pipelines for This Use Case

Here we will describe all Kedro pipelines available for `recommender_gnn` use case.

*Note that these are names of **Kedro pipeline modules**, which are defined in `recommender_gnn/src/recommender_gnn/pipelines` and **not initialized pipeline objects**. To initialize pipeline with use of Kedro pipeline module you must registered it in `src/recommender_gnn/pipeline_registry.py` file.*

### Data Preparing

- `<dataset>_preprocessing`
    - pipeline where all preprocessing steps for given `dataset` should take place. Can include: sampling, filtering, missing data imputing or any other required transformations on raw data,
    - example `otto_preprocessing` pipeline was implemented.
- `<dataset>_to_act`
    - pipeline in which `dataset` should be transformed into aforementioned **ACT** format,
    - should be run on data preprocessed with `<dataset>_preprocessing` pipeline,
    - example `otto_to_act` pipeline was implemented.

### Preprocessing Data to Graph Format

- `graph_recommendation_preprocessing`
    - preprocesses data from **ACT** format into tabular format commonly required by recommendation GNN models,
- `graph_recommendation_modelling` 
    - if it is required by given GNN model, converts tabular data returned by `graph_recommendation_preprocessing` pipeline into graphs which are input to GNN model,
    - can be considered as a feature extraction step

### Generating Recommendations

- `graph_recommendation`
    - trains GNN recommendation model, logs loss, recall, ndcg metrics on **train** and **validation** subsets of data and saves trained model to `MLflow`,
    - runs trained model on **test** subset of data and logs loss, recall and ndcg metrics to `MLflow`,
    - uses trained model to make inference on **predict** subset of data and saves recommendations to csv file.

### Generating Kaggle Submission

- `kaggle_submission`
    - generates generic Kaggle submission file from saved recommendations,
    - for different Kaggle competitions, it is possible that a slightly different format will be required.

### Supplementary Pipelines

- `test_gpu`
    - utility pipeline for testing GPU configuration in the cloud.

## How to Run Pipelines in Your Environment

[Kedro](https://github.com/kedro-org/kedro) framework was used to create maintainable and modular data science code. Take a look at the Kedro [documentation](https://kedro.readthedocs.io) to get started. Before executing any commands for this use case you should change your working directory to `recommender_gnn`.

Kedro pipelines should be run in order defined in [Kedro Pipelines](#kedro-pipelines) section. All available initialized pipelines are registered in `src/recommender_gnn/pipeline_registry.py` file, where you can find **pipeline names**. Each of these registered pipelines can be run with following command:

```bash
kedro run --pipeline <pipeline_name>
```

Example:

```bash
kedro run --pipeline otto_graph_recommendation_preprocessing
```

Kedro pipelines are built from individual steps, which are called nodes. If you would like to **run only individual nodes** from a given pipeline you can use the command: 

```bash
kedro run --pipeline <pipeline_name> --node <namespace_name>.<node_name>
```

Example:

```bash
kedro run --pipeline otto_graph_recommendation_preprocessing --node graph_recommendation_preprocessing_otto.map_users_and_items_node
```

**Namespace and node names** can be found in `recommender_gnn/src/recommender_gnn/pipelines/<pipeline_name>/pipeline.py` file. Kedro namespaces is feature introduced to simplify creation of [modular pipelines](https://kedro.readthedocs.io/en/stable/tutorial/add_another_pipeline.html#optional-modular-pipelines). If you are not using modular pipeline you do not need to specify `<namespace_name>`.

Each pipeline has also specified default input and output datasets names. You can override this settings by running pipelines with options `--from-inputs` or `--to-outputs`.

Example:

```bash
kedro run --pipeline otto_graph_recommendation_preprocessing --from-inputs otto_train_parquet
```

## How to Run Pipelines on GCP Vertex AI Pipelines

[kedro-vertexai](https://github.com/getindata/kedro-vertexai) is one of kedro plugins that supports running workflows on GCP Vertex AI Pipelines.
You can specify configuration of such workflows inside `conf/base/vertexai.yml` file. After that it is very easy to run
your Kedro pipelines in the cloud. To start Vertex AI Pipeline, run `kedro vertexai --env cloud run-once --pipeline <pipeline_name>`.

### Example GPU Configuration

Let's take a look at some of the contents of example `kedro-vertexai` configuration file in `conf/base/vertexai.yml`:

```yaml
# ...

  resources:

    # For nodes that require more RAM you can increase the "memory"
    data-import-node:
      memory: 2Gi

    # Training nodes can utilize more than one CPU if the algorithm
    # supports it
    model-training-node:
      cpu: 8
      memory: 60Gi

    # GPU-capable nodes can request 1 GPU slot
    tensorflow-node:
      gpu: 1

    # Resources can be also configured via nodes tag
    # (if there is node name and tag configuration for the same
    # resource, tag configuration is overwritten with node one)
    gpu_node_tag:
      cpu: 1
      gpu: 2

    # Default settings for the nodes
    __default__:
      cpu: 200m
      memory: 64Mi

  # Optional section allowing to configure node selectors constraints
  # like gpu accelerator for nodes with gpu resources.
  # (Note that not all accelerators are available in all
  # regions - https://cloud.google.com/compute/docs/gpus/gpu-regions-zones)
  # and not for all machines and resources configurations - 
  # https://cloud.google.com/vertex-ai/docs/training/configure-compute#specifying_gpus
  node_selectors:
    gpu_node_tag:
      cloud.google.com/gke-accelerator: NVIDIA_TESLA_T4
    tensorflow-step:
      cloud.google.com/gke-accelerator: NVIDIA_TESLA_K80
      
# ...
```

As presented above `resources` and `node_selectors` sections enable adjustment of the resources reservations and limits for the
selected `Kedro` nodes. Settings for individual nodes, we can define in two ways - using the name of the node or
its [tag](https://kedro.readthedocs.io/en/stable/nodes_and_pipelines/nodes.html#how-to-tag-a-node) (if there is node name and tag configuration for the same resource, tag configuration is overwritten with
node one). For example, with the `vertexai.yaml` configuration file shown above and the `Kedro`
pipeline containing such a node:

```python
def create_pipeline(**kwargs):
    return Pipeline(
        [
            node(
                func=train_model,
                inputs=["X_train", "y_train"],
                outputs="regressor",
                name="model_training_node",
                tags="gpu_node_tag",
            ),
        ]
    )
```

we expect this particular node to run with two `NVIDIA_TESLA_T4` GPUs, eight CPUs, and memory allocated according to
the specified `60Gi` limit. (Not all accelerators are available in all [regions](https://cloud.google.com/compute/docs/gpus/gpu-regions-zones) and not for all [machines and
resources configurations](https://cloud.google.com/vertex-ai/docs/training/configure-compute#specifying_gpus))

*Note that if your node is part of modular pipeline you need to specify its name in `<namespace_name>.<node_name>`
convention. It is not required for tags.*

## Kedro

### Rules and Guidelines

* Don't commit any credentials or your local configuration to your repository. Keep all your credentials and local configuration in `conf/local/`


### How to Run Kedro

You can run your Kedro project with:

```bash
kedro run
```
> by default, Kedro runs the `__default__` pipeline from `src/pipeline_registry.py`

To run a specific pipeline:
```bash
kedro run -p "<PIPELINE_NAME>"
```

To see all the possible flags/options, run `kedro run -h`

### How to Work With the Project Interactively?

```bash
kedro jupyter notebook
kedro jupyter lab
kedro ipython
```
To work with notebooks, you can load `context`, `catalog` and `startup_error` variables by running following cells:
```python
%load_ext kedro.extras.extensions.ipython
%reload_kedro
```

After loading, you can access **datasets** and **parameters** with `context`:
```python
context.catalog.load('name_of_dataset_from_data_catalog')
```

To see all datasets, run:
```python
[data for data in catalog.list() if not data.startswith('params:')]
```

### Kedro Project Structure
- use case implemented within QuickStart ML Blueprints can be found inside `<use_case_name>` directory
- Kedro uses the pipeline design pattern, each pipeline is located inside `src/<use_case_name>/pipelines` directory
- each pipeline has optional YAML parameters, which you can specify inside `conf/<ENV>/parameters` directory
- to create a new pipeline and run, you must:
    - create new pipeline `kedro pipeline create <NEW_PIPELINE_NAME>`
    - add recently created pipeline to `src/pipeline_registry.py`
    - run new pipeline `kedro run -p <NEW_PIPELINE_NAME>`
- each pipeline consists of `nodes.py` and `pipeline.py` files
    - basically, you can think of nodes as of building blocks that can be combined in pipelines to build workflows
    - inside `nodes.py` there are Python functions to be executed within a particular pipeline
    - inside `pipeline.py` you can specify the inputs, outputs, functions and parameters of a pipeline
    - by default, Kedro uses `SequentialRunner`, so pipelines are executed sequentially. There must not be any circular dependencies
- you can specify dataset location inside `conf/<ENV>/catalog.yml` file
- you can keep your local data inside `data/` directory, by default all data files are in `.gitignore` so you can't commit them to your repository

### Custom Kedro Objects
You can implement your own custom Kedro objects:
- datasets - to be used within `conf/<ENV>/catalog.yml`
    - implement your dataset class inside `src/<use_case_name>/extras/datasets` directory
    - at the minimum, a valid Kedro dataset needs to subclass the base `AbstractDataSet` and provide an implementation for the following abstract methods: `_load`, `_save` and `_describe`
- hooks - can be used before/after specific execution points of project
    - examples of hooks: `before_node_run`, `after_node_run`, `on_node_error`
    - you can write custom hooks inside `src/<use_case_name>/hooks.py` file
    - to register a hook, you have to import and add it to `HOOKS` tuple inside `src/<use_case_name>/settings.py` file
- runners - execution mechanisms used to run pipelines
    - examples of runners: `SequentialRunner` (default), `ParallelRunner`

To see all available custom Kedro objects and examples, check out Kedro official documentation.

---
## Other Kedro Plugins
### [Kedro-Viz](https://github.com/kedro-org/kedro-viz)
- visualizes Kedro pipelines in an informative way
- to run, `kedro viz --autoreload` inside project's directory
- this will run a server on `http://127.0.0.1:4141`

### [kedro-docker](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-docker)
Before running, make sure you have:
- installed Docker
- Docker daemon up and running

In order to build an image, run:
```bash
kedro docker build
```
> If you have a MacBook with ARM processor, you must add `--docker-args=--platform=linux/amd64` option to run docker in the cloud.

Once built, you can run your project in a Docker environment:
```bash
kedro docker run
```
> This command automatically adds `--rm` flag, so the container will be automatically removed when it exits.

You can run your Docker image interactively:
```bash
kedro docker ipython
kedro docker jupyter notebook
kedro docker jupyter lab
```

`kedro-docker` allows to analyze the size efficiency of your project image by leveraging `dive`:
```bash
kedro docker dive
```

### [kedro-mlflow](https://github.com/Galileo-Galilei/kedro-mlflow)
- lightweight integration of `MLflow` inside `Kedro` projects
- configuration can be specified inside `conf/<ENV>/mlflow.yml` file
- by default, experiments are saved inside `mlruns` local directory
- to see all the local experiments, run `kedro mlflow ui`