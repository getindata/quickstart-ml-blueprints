# GetInData ML Framework - Graph Neural Networks For Sequential Recommendations

## Overview

`recommender_gnn` is one of use cases implemented within GetInData ML Framework. The purpose of this use case is to
demonstrate the utilization of an example graph network that achieves state-of-the-art results for a recommendation task
on sequential data. It can also be used as a guide on how to train models using GPU accelerators on Google Cloud
Platform employing the [kedro-vertexai plugin](https://github.com/getindata/kedro-vertexai).

## Data

As this case focuses on sequential recommendation data, we can use any data of this type if it is properly preprocessed.
An example preprocessing pipeline was implemented for [OTTO â€“ Multi-Objective Recommender System](https://www.kaggle.com/competitions/otto-recommender-system)
dataset from kaggle competition. It is also possible to utilize transaction data from [H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations) dataset, which are preprocessed as 
part of `recommender_ranking` use case. In the general case, the data we want to use as an input, should be preprocessed
in accordance to following example:

| customer_id | article_id | timestamp  |
|-------------|------------|------------|
| 1215        | shoes      | 1670405262 |
| 1220        | hat        | 1651405345 |
| 2600        | t-shirt    | 1631445222 |

which is called `transactions` table and should include data about every interaction in our recommendation dataset. Apart
from transactions these can be for example clicks generated by users or additions to website cart. Each such interaction
in this format should be described with customer (or for example session) identifier in
`customer_id` column, article identifier in `article_id` column, and time of an interaction in `timestamp` if it is an 
integer or `date` column if it is in date format. Apart from `transactions` table there should also be present
`customers` and `articles` table with unique customers and articles identifiers respectively. In this use case such a 
format is called **ACT** (Articles, Customers, Transactions).

## Models

For now examples are implemented with use of [Dynamic Graph Neural Networks for Sequential 
Recommendation](https://github.com/ZM7/DGSR) (DGSR) (Zhang et al., [2022](https://doi.org/10.1109/TKDE.2022.3151618)). This
algorithm utilize ideas from **dynamic graph neural networks** to model the dynamic collaborative signals among all
interactions in data, which helps in exploring the interactive behavior of users and items with time and order
information. Two main parts of aforementioned network are **attention module**, which is constructed to capture the long-term
preference of users and long-term character of items, and a **recurrent neural module**, which is further utilized to learn
short-term preference and character of users and items, respectively.

![dgsr_architecture](https://user-images.githubusercontent.com/32554595/205959876-25f6e4fa-4a32-4a79-9cff-77e8f5656077.png)

## Kedro Pipelines

Here we will describe all Kedro pipelines available for `recommender_gnn` use case.

*Note that these are names of **Kedro pipeline modules**, which are defined in `recommender_gnn/src/recommender_gnn/pipelines` and **not initialized pipeline objects**. To initialize pipeline with use of Kedro pipeline module you must registered it in `src/recommender_gnn/pipeline_registry.py` file.*

### Data Preparing

- `<dataset>_preprocessing`
    - pipeline where all preprocessing steps for given `dataset` should take place. Can include: sampling, filtering, missing data imputing or any other required transformations on raw data,
    - example `otto_preprocessing` pipeline was implemented.
- `<dataset>_to_act`
    - pipeline in which `dataset` should be transformed into aforementioned **ACT** format,
    - should be run on data preprocessed with `<dataset>_preprocessing` pipeline,
    - example `otto_to_act` pipeline was implemented.

### Preprocessing Data to Graph Format

- `graph_recommendation_preprocessing`
    - preprocesses data from **ACT** format into tabular format commonly required by recommendation GNN models,
- `graph_recommendation_modelling` 
    - if it is required by given GNN model, converts tabular data returned by `graph_recommendation_preprocessing` pipeline into graphs which are input to GNN model,
    - can be considered as a feature extraction step

### Generating Recommendations

- `graph_recommendation`
    - trains GNN recommendation model, logs loss, recall, ndcg metrics on **train** and **validation** subsets of data and saves trained model to `MLflow`,
    - runs trained model on **test** subset of data and logs loss, recall and ndcg metrics to `MLflow`,
    - uses trained model to make inference on **predict** subset of data and saves recommendations to csv file.

### Generating Kaggle Submission

- `kaggle_submission`
    - generates generic Kaggle sumbission file from saved recommendations,
    - for different Kaggle competitions, it is possible that a slightly different format will be required.



## How to Run

[Kedro](https://github.com/kedro-org/kedro) framework was used to create maintainable and modular data science code. Take a look at the Kedro [documentation](https://kedro.readthedocs.io) to get started.

Kedro pipelines should be run in order defined in [Kedro Pipelines](#kedro-pipelines) section. All available initialized pipelines are registered in `src/recommender_gnn/pipeline_registry.py` file, where you can find **pipeline names**. Each of these registered pipelines can be run with following command:

```bash
kedro run --pipeline <pipeline_name>
```

Example:

```bash
kedro run --pipeline otto_graph_recommendation_preprocessing
```

Kedro pipelines are built from individual steps, which are called nodes. If you would like to **run only individual nodes** from a given pipeline you can use the command: 

```bash
kedro run --pipeline <pipeline_name> --node <namespace_name>.<node_name>
```

Example:

```bash
kedro run --pipeline otto_graph_recommendation_preprocessing --node graph_recommendation_preprocessing_otto_ map_users_and_items_node
```

**Namespace and node names** can be found in `recommender_gnn/src/recommender_gnn/pipelines/<pipeline_name>/pipeline.py` file. Kedro namespaces is feature introduced to simplify creation of [modular pipelines](https://kedro.readthedocs.io/en/stable/tutorial/add_another_pipeline.html#optional-modular-pipelines). If you are not using modular pipeline you do not need to specify `<namespace_name>`.

Each pipeline has also specified default input and output datasets names. You can override this settings by running pipelines with options `--from-inputs` or `--to-outputs`.

Example:

```bash
kedro run --pipeline otto_graph_recommendation_preprocessing --from-inputs otto_train_parquet
```

## Local Setup

### Data

You can download `Otto` dataset with optimized memory footprint in parquet format from [here](Otto Full Optimized Memory Footprint).
Downloaded files should be placed in `recommender_gnn/data/02_intermediate/otto` directory.

You can also use `H&M` dataset ...

### Docker

...


## Kedro

### Rules and guidelines

* Don't commit any credentials or your local configuration to your repository. Keep all your credentials and local configuration in `conf/local/`


### How to run Kedro

You can run your Kedro project with:

```bash
kedro run
```
> by default, Kedro runs the `__default__` pipeline from `src/pipeline_registry.py`

To run a specific pipeline:
```bash
kedro run -p "<PIPELINE_NAME>"
```

To see all the possible flags/options, run `kedro run -h`

### How to work with the project interactively?

```bash
kedro jupyter notebook
kedro jupyter lab
kedro ipython
```
To work with notebooks, you can load `context`, `catalog` and `startup_error` variables by running following cells:
```python
%load_ext kedro.extras.extensions.ipython
%reload_kedro
```

After loading, you can access **datasets** and **parameters** with `context`:
```python
context.catalog.load('name_of_dataset_from_data_catalog')
```

To see all datasets, run:
```python
[data for data in catalog.list() if not data.startswith('params:')]
```

### Kedro project structure
- use case implemented within GetInData ML Framework can be found inside `<use_case_name>` directory
- Kedro uses the pipeline design pattern, each pipeline is located inside `src/<use_case_name>/pipelines` directory
- each pipeline has optional YAML parameters, which you can specify inside `conf/<ENV>/parameters` directory
- to create a new pipeline and run, you must:
    - create new pipeline `kedro pipeline create <NEW_PIPELINE_NAME>`
    - add recently created pipeline to `src/pipeline_registry.py`
    - run new pipeline `kedro run -p <NEW_PIPELINE_NAME>`
- each pipeline consists of `nodes.py` and `pipeline.py` files
    - basically, you can think of nodes as of building blocks that can be combined in pipelines to build workflows
    - inside `nodes.py` there are Python functions to be executed within a particular pipeline
    - inside `pipeline.py` you can specify the inputs, outputs, functions and parameters of a pipeline
    - by default, Kedro uses `SequentialRunner`, so pipelines are executed sequentially. There must not be any circular dependencies
- you can specify dataset location inside `conf/<ENV>/catalog.yml` file
- you can keep your local data inside `data/` directory, by default all data files are in `.gitignore` so you can't commit them to your repository
- 

### Custom Kedro objects
You can implement your own custom Kedro objects:
- datasets - to be used within `conf/<ENV>/catalog.yml`
    - implement your dataset class inside `src/<use_case_name>/extras/datasets` directory
    - at the minimum, a valid Kedro dataset needs to subclass the base `AbstractDataSet` and provide an implementation for the following abstract methods: `_load`, `_save` and `_describe`
- hooks - can be used before/after specific execution points of project
    - examples of hooks: `before_node_run`, `after_node_run`, `on_node_error`
    - you can write custom hooks inside `src/<use_case_name>/hooks.py` file
    - to register a hook, you have to import and add it to `HOOKS` tuple inside `src/<use_case_name>/settings.py` file
- runners - execution mechanisms used to run pipelines
    - examples of runners: `SequentialRunner` (default), `ParallelRunner`

To see all available custom Kedro objects and examples, check out Kedro official documentation.

---
## Kedro plugins
### [Kedro-Viz](https://github.com/kedro-org/kedro-viz)
- visualizes Kedro pipelines in an informative way
- to run, `kedro viz --autoreload` inside project's directory
- this will run a server on `http://127.0.0.1:4141`


### [kedro-docker](https://github.com/kedro-org/kedro-plugins/tree/main/kedro-docker)
Before running, make sure you have:
- installed Docker
- Docker daemon up and running

In order to build an image, run:
```bash
kedro docker build
```
> If you have a MacBook with ARM processor, you must add `--docker-args=--platform=linux/amd64` option to run docker in the cloud.

Once built, you can run your project in a Docker environment:
```bash
kedro docker run
```
> This command automatically adds `--rm` flag, so the container will be automatically removed when it exits.

You can run your Docker image interactively:
```bash
kedro docker ipython
kedro docker jupyter notebook
kedro docker jupyter lab
```

`kedro-docker` allows to analyze the size efficiency of your project image by leveraging `dive`:
```bash
kedro docker dive
```

### [kedro-mlflow](https://github.com/Galileo-Galilei/kedro-mlflow)
- lightweight integration of `MLflow` inside `Kedro` projects
- configuration can be specified inside `conf/<ENV>/mlflow.yml` file
- by default, experiments are saved inside `mlruns` local directory
- to see all the local experiments, run `kedro mlflow ui`

### [kedro-vertexai](https://github.com/getindata/kedro-vertexai)
- supports running workflows on GCP Vertex AI Pipelines
- configuration can be specified inside `conf/<ENV>/vertexai.yml` file
- to start Vertex AI Pipeline, run `kedro vertexai -e <ENV> run-once -p <PIPELINE_NAME>`